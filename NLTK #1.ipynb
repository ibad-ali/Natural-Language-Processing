{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK (1: Tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anaconda is a package manager, an environment manager, and Python distribution that contains a collection of many open source packages.\n",
      "\n",
      "This is advantageous as when you are working on a data science project, you will find that you need many different packages (numpy, scikit-learn, scipy, pandas to name a few), which an installation of Anaconda comes preinstalled with.\n",
      "\n",
      "If you need additional packages after installing Anaconda, you can use Anacondas package manager, conda, or pip to install those packages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "text = 'Anaconda is a package manager, an environment manager, and Python distribution that contains a collection of many open source packages. This is advantageous as when you are working on a data science project, you will find that you need many different packages (numpy, scikit-learn, scipy, pandas to name a few), which an installation of Anaconda comes preinstalled with. If you need additional packages after installing Anaconda, you can use Anacondas package manager, conda, or pip to install those packages.'\n",
    "for i in sent_tokenize(text):\n",
    "    print(i)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anaconda\n",
      "is\n",
      "a\n",
      "package\n",
      "manager\n",
      ",\n",
      "an\n",
      "environment\n",
      "manager\n",
      ",\n",
      "and\n",
      "Python\n",
      "distribution\n",
      "that\n",
      "contains\n",
      "a\n",
      "collection\n",
      "of\n",
      "many\n",
      "open\n",
      "source\n",
      "packages\n",
      ".\n",
      "This\n",
      "is\n",
      "advantageous\n",
      "as\n",
      "when\n",
      "you\n",
      "are\n",
      "working\n",
      "on\n",
      "a\n",
      "data\n",
      "science\n",
      "project\n",
      ",\n",
      "you\n",
      "will\n",
      "find\n",
      "that\n",
      "you\n",
      "need\n",
      "many\n",
      "different\n",
      "packages\n",
      "(\n",
      "numpy\n",
      ",\n",
      "scikit-learn\n",
      ",\n",
      "scipy\n",
      ",\n",
      "pandas\n",
      "to\n",
      "name\n",
      "a\n",
      "few\n",
      ")\n",
      ",\n",
      "which\n",
      "an\n",
      "installation\n",
      "of\n",
      "Anaconda\n",
      "comes\n",
      "preinstalled\n",
      "with\n",
      ".\n",
      "If\n",
      "you\n",
      "need\n",
      "additional\n",
      "packages\n",
      "after\n",
      "installing\n",
      "Anaconda\n",
      ",\n",
      "you\n",
      "can\n",
      "use\n",
      "Anacondas\n",
      "package\n",
      "manager\n",
      ",\n",
      "conda\n",
      ",\n",
      "or\n",
      "pip\n",
      "to\n",
      "install\n",
      "those\n",
      "packages\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for i in word_tokenize(text):\n",
    "    print(i)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK (2: Stop Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'most', 'she', 'myself', \"couldn't\", 've', 'under', 'will', 'weren', 'hers', \"hadn't\", 'didn', 'hadn', 'am', 'the', 'he', \"shouldn't\", 'just', 'been', 'so', 'should', 'my', 'to', 'isn', 'do', 'does', 'don', 'until', \"aren't\", 'wouldn', 'because', 'our', 'where', 'of', \"won't\", 'll', 'few', 'for', 'by', 'between', \"didn't\", 'but', \"you'll\", 'them', 'through', 'or', 'against', 'after', 'all', 'and', 'at', 'while', 'his', 'once', 'yourself', 'aren', 'with', 'some', 'too', 'yours', 'those', 'before', 'hasn', 'm', 'why', 'how', \"shan't\", 'when', \"you've\", 'yourselves', 'it', 'into', 'ma', 'themselves', \"wouldn't\", 'not', 'below', 'other', 'can', 'has', 'nor', 'whom', 'doing', \"don't\", 'now', \"doesn't\", 'only', 'couldn', 'have', 'did', 'shouldn', 'be', 'during', 'up', 'above', 'own', \"you're\", 'that', 'each', \"haven't\", \"it's\", 'are', 'both', 'down', 'then', \"hasn't\", 'there', 'such', \"mightn't\", 'out', 'itself', 'a', 'than', 'himself', 'you', \"weren't\", \"you'd\", 'these', \"that'll\", 'on', 'mustn', 'we', 'doesn', \"isn't\", 'over', 'again', 'same', 'her', 'theirs', 'in', 'ain', 'needn', 'having', 's', 't', 'had', 'about', \"she's\", \"needn't\", 'who', 'as', \"wasn't\", 'more', 'this', 're', 'its', \"mustn't\", 'y', 'me', 'any', 'haven', 'is', 'what', 'further', \"should've\", 'were', 'an', 'your', 'won', 'off', 'shan', 'being', 'here', 'they', 'i', 'very', 'mightn', 'which', 'd', 'wasn', 'their', 'from', 'o', 'was', 'if', 'no', 'him', 'ours', 'herself', 'ourselves'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "print(stop_words) \n",
    "\n",
    "#these words are the stop words in english language by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', 'upload', 'commit', 'existing', 'file', 'GitHub', 'repository', '.', 'Drag', 'drop', 'file', 'directory', 'file', 'tree', ',', 'upload', 'files', 'repository', \"'s\", 'main', 'page', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"You can upload and commit an existing file to a GitHub repository. Drag and drop a file to any directory in the file tree, or upload files from the repository's main page.\"\n",
    "text_words = word_tokenize(text)\n",
    "result_word = []\n",
    "for i in text_words:\n",
    "    if i not in stop_words:\n",
    "        result_word.append(i)\n",
    "print(result_word)\n",
    "\n",
    "#output show the text_words with out stops words or we just remove the stop words in our text\n",
    "\n",
    "# print(len(text_words))\n",
    "# print(len(result_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK (3: Stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ali\n",
      "play\n",
      "the\n",
      "cricket\n",
      "today\n",
      "while\n",
      "it\n",
      "wa\n",
      "play\n",
      "from\n",
      "hi\n",
      "baat\n",
      "the\n",
      "match\n",
      "ha\n",
      "been\n",
      "lost\n",
      "that\n",
      "play\n",
      "by\n",
      "ali\n",
      "'s\n",
      "team\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "text = \"Ali plays the cricket today while it was playing from his baat the match has been lost that played by ali's team\"\n",
    "words = word_tokenize(text)\n",
    "\n",
    "for i in words:\n",
    "    print(ps.stem(i))\n",
    "    \n",
    "# in output the text contain three words plays, played and playing stem convert it into a one word play means it convert roots form words into a word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK (4: Tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Anaconda', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('package', 'NN'), ('manager', 'NN'), (',', ','), ('an', 'DT'), ('environment', 'NN'), ('manager', 'NN'), (',', ','), ('and', 'CC'), ('Python', 'NNP'), ('distribution', 'NN'), ('that', 'WDT'), ('contains', 'VBZ'), ('a', 'DT'), ('collection', 'NN'), ('of', 'IN'), ('many', 'JJ'), ('open', 'JJ'), ('source', 'NN'), ('packages', 'NNS'), ('.', '.'), ('This', 'DT'), ('is', 'VBZ'), ('advantageous', 'JJ'), ('as', 'IN'), ('when', 'WRB'), ('you', 'PRP'), ('are', 'VBP'), ('working', 'VBG'), ('on', 'IN'), ('a', 'DT'), ('data', 'NN'), ('science', 'NN'), ('project', 'NN'), (',', ','), ('you', 'PRP'), ('will', 'MD'), ('find', 'VB'), ('that', 'IN'), ('you', 'PRP'), ('need', 'VBP'), ('many', 'JJ'), ('different', 'JJ'), ('packages', 'NNS'), ('(', '('), ('numpy', 'JJ'), (',', ','), ('scikit-learn', 'JJ'), (',', ','), ('scipy', 'JJ'), (',', ','), ('pandas', 'JJ'), ('to', 'TO'), ('name', 'VB'), ('a', 'DT'), ('few', 'JJ'), (')', ')'), (',', ','), ('which', 'WDT'), ('an', 'DT'), ('installation', 'NN'), ('of', 'IN'), ('Anaconda', 'NNP'), ('comes', 'VBZ'), ('preinstalled', 'VBN'), ('with', 'IN'), ('.', '.'), ('If', 'IN'), ('you', 'PRP'), ('need', 'VBP'), ('additional', 'JJ'), ('packages', 'NNS'), ('after', 'IN'), ('installing', 'VBG'), ('Anaconda', 'NNP'), (',', ','), ('you', 'PRP'), ('can', 'MD'), ('use', 'VB'), ('Anacondas', 'NNP'), ('package', 'NN'), ('manager', 'NN'), (',', ','), ('conda', 'NN'), (',', ','), ('or', 'CC'), ('pip', 'NN'), ('to', 'TO'), ('install', 'VB'), ('those', 'DT'), ('packages', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "text = 'Anaconda is a package manager, an environment manager, and Python distribution that contains a collection of many open source packages. This is advantageous as when you are working on a data science project, you will find that you need many different packages (numpy, scikit-learn, scipy, pandas to name a few), which an installation of Anaconda comes preinstalled with. If you need additional packages after installing Anaconda, you can use Anacondas package manager, conda, or pip to install those packages.'\n",
    "words = nltk.word_tokenize(text)\n",
    "print(nltk.pos_tag(words))\n",
    "#POS-tag, processes a sequence of words and attaches a part of speech tag to each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Anaconda', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('package', 'NN'), ('manager', 'NN'), (',', ','), ('an', 'DT'), ('environment', 'NN'), ('manager', 'NN'), (',', ','), ('and', 'CC'), ('Python', 'NNP'), ('distribution', 'NN'), ('that', 'WDT'), ('contains', 'VBZ'), ('a', 'DT'), ('collection', 'NN'), ('of', 'IN'), ('many', 'JJ'), ('open', 'JJ'), ('source', 'NN'), ('packages', 'NNS'), ('.', '.')]\n",
      "[('This', 'DT'), ('is', 'VBZ'), ('advantageous', 'JJ'), ('as', 'IN'), ('when', 'WRB'), ('you', 'PRP'), ('are', 'VBP'), ('working', 'VBG'), ('on', 'IN'), ('a', 'DT'), ('data', 'NN'), ('science', 'NN'), ('project', 'NN'), (',', ','), ('you', 'PRP'), ('will', 'MD'), ('find', 'VB'), ('that', 'IN'), ('you', 'PRP'), ('need', 'VBP'), ('many', 'JJ'), ('different', 'JJ'), ('packages', 'NNS'), ('(', '('), ('numpy', 'JJ'), (',', ','), ('scikit-learn', 'JJ'), (',', ','), ('scipy', 'JJ'), (',', ','), ('pandas', 'JJ'), ('to', 'TO'), ('name', 'VB'), ('a', 'DT'), ('few', 'JJ'), (')', ')'), (',', ','), ('which', 'WDT'), ('an', 'DT'), ('installation', 'NN'), ('of', 'IN'), ('Anaconda', 'NNP'), ('comes', 'VBZ'), ('preinstalled', 'VBN'), ('with', 'IN'), ('.', '.')]\n",
      "[('If', 'IN'), ('you', 'PRP'), ('need', 'VBP'), ('additional', 'JJ'), ('packages', 'NNS'), ('after', 'IN'), ('installing', 'VBG'), ('Anaconda', 'NNP'), (',', ','), ('you', 'PRP'), ('can', 'MD'), ('use', 'VB'), ('Anacondas', 'NNP'), ('package', 'NN'), ('manager', 'NN'), (',', ','), ('conda', 'NN'), (',', ','), ('or', 'CC'), ('pip', 'NN'), ('to', 'TO'), ('install', 'VB'), ('those', 'DT'), ('packages', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "train_text = 'Anaconda is a package manager, an environment manager, and Python distribution that contains a collection of many open source packages. This is advantageous as when you are working on a data science project, you will find that you need many different packages (numpy, scikit-learn, scipy, pandas to name a few), which an installation of Anaconda comes preinstalled with. If you need additional packages after installing Anaconda, you can use Anacondas package manager, conda, or pip to install those packages.'\n",
    "test_text = 'When you create a new environment, Navigator installs the same Python version you used when you downloaded and installed Anaconda. If you want to use a different version of Python, for example Python 3.5, simply create a new environment and specify the version of Python that you want in that environment.'\n",
    "sent_tokenizer = nltk.PunktSentenceTokenizer(train_text)\n",
    "tokenized = sent_tokenize.tokenize(train_text)\n",
    "for i in tokenized:\n",
    "    words = nltk.word_tokenize(i)\n",
    "    print(nltk.pos_tag(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('RB') #this is the way how we get info about the part of speech word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('PRP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('NNS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''POS tag list:\n",
    "\n",
    "CC\tcoordinating conjunction\n",
    "CD\tcardinal digit\n",
    "DT\tdeterminer\n",
    "EX\texistential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "FW\tforeign word\n",
    "IN\tpreposition/subordinating conjunction\n",
    "JJ\tadjective\t'big'\n",
    "JJR\tadjective, comparative\t'bigger'\n",
    "JJS\tadjective, superlative\t'biggest'\n",
    "LS\tlist marker\t1)\n",
    "MD\tmodal\tcould, will\n",
    "NN\tnoun, singular 'desk'\n",
    "NNS\tnoun plural\t'desks'\n",
    "NNP\tproper noun, singular\t'Harrison'\n",
    "NNPS\tproper noun, plural\t'Americans'\n",
    "PDT\tpredeterminer\t'all the kids'\n",
    "POS\tpossessive ending\tparent\\'s\n",
    "PRP\tpersonal pronoun\tI, he, she\n",
    "PRP$\tpossessive pronoun\tmy, his, hers\n",
    "RB\tadverb\tvery, silently,\n",
    "RBR\tadverb, comparative\tbetter\n",
    "RBS\tadverb, superlative\tbest\n",
    "RP\tparticle\tgive up\n",
    "TO\tto\tgo 'to' the store.\n",
    "UH\tinterjection\terrrrrrrrm\n",
    "VB\tverb, base form\ttake\n",
    "VBD\tverb, past tense\ttook\n",
    "VBG\tverb, gerund/present participle\ttaking\n",
    "VBN\tverb, past participle\ttaken\n",
    "VBP\tverb, sing. present, non-3d\ttake\n",
    "VBZ\tverb, 3rd person sing. present\ttakes\n",
    "WDT\twh-determiner\twhich\n",
    "WP\twh-pronoun\twho, what\n",
    "WP$\tpossessive wh-pronoun\twhose\n",
    "WRB\twh-abverb\twhere, when'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK (5: Chuncking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'eat', 'pasta', 'yesterday']\n",
      "(S I/PRP eat/VBP pasta/NN yesterday/NN)\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "text = \"I eat pasta yesterday\"\n",
    "words = nltk.word_tokenize(text)\n",
    "print(words)\n",
    "tagged = nltk.pos_tag(words)\n",
    "chunks = nltk.chunk.ne_chunk(tagged)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  GreyAtom/NNP\n",
      "  is/VBZ\n",
      "  committed/VBN\n",
      "  to/TO\n",
      "  building/VBG\n",
      "  (NP an/DT educational/JJ ecosystem/NN)\n",
      "  for/IN\n",
      "  learners/NNS\n",
      "  to/TO\n",
      "  upskill/VB\n",
      "  &/CC\n",
      "  help/VB\n",
      "  them/PRP\n",
      "  make/VB\n",
      "  (NP a/DT career/NN)\n",
      "  in/IN\n",
      "  data/NNS\n",
      "  (NP science/NN)\n",
      "  ./.)\n",
      "(NP an/DT educational/JJ ecosystem/NN)\n",
      "(NP a/DT career/NN)\n",
      "(NP science/NN)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "sentence = \"GreyAtom is committed to building an educational ecosystem for learners to upskill & help them make a career in data science.\"\n",
    "grammar = ('''                   \n",
    "    NP: {<DT>?<JJ>*<NN>} # NP\n",
    "    ''')\n",
    "\n",
    "tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "\n",
    "chunk_pars = nltk.RegexpParser(grammar)\n",
    "tree = chunk_pars.parse(tagged)\n",
    "for subtree in tree.subtrees():\n",
    "    print(subtree)\n",
    "\n",
    "#tree.draw()\n",
    "\n",
    "#Chunking in Natural Language Processing (NLP) is the process by which we group various words together by their part of speech tags. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK (6: Chinking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NP\n",
      "  GreyAtom/NNP\n",
      "  is/VBZ\n",
      "  committed/VBN\n",
      "  to/TO\n",
      "  building/VBG\n",
      "  an/DT\n",
      "  educational/JJ\n",
      "  ecosystem/NN)\n",
      "('for', 'IN')\n",
      "(NP\n",
      "  learners/NNS\n",
      "  to/TO\n",
      "  upskill/VB\n",
      "  &/CC\n",
      "  help/VB\n",
      "  them/PRP\n",
      "  make/VB\n",
      "  a/DT\n",
      "  career/NN)\n",
      "('in', 'IN')\n",
      "(NP data/NNS science/NN ./.)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "grammar = r\"\"\"\n",
    "    NP:\n",
    "    {<.*>+}          # Chunk everything\n",
    "    }<VBD|IN>+{      # Chink sequences of VBD and IN\n",
    "  \"\"\"\n",
    "sentence = 'GreyAtom is committed to building an educational ecosystem for learners to upskill & help them make a career in data science.'\n",
    "\n",
    "tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(tagged)\n",
    "for r in result:\n",
    "    print(r)\n",
    "    \n",
    "# result.draw()\n",
    "\n",
    "#Chinking is a lot like chunking, it is basically a way for you to remove a chunk from a chunk. \n",
    "#The chunk that you remove from your chunk is your chink."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK (6: Name Entity Recognition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Mark/NNP)\n",
      "  and/CC\n",
      "  (PERSON John/NNP)\n",
      "  are/VBP\n",
      "  working/VBG\n",
      "  at/IN\n",
      "  (ORGANIZATION Google/NNP)\n",
      "  ./.)\n",
      "[('Mark', 'NNP', 'B-PERSON'), ('and', 'CC', 'O'), ('John', 'NNP', 'B-PERSON'), ('are', 'VBP', 'O'), ('working', 'VBG', 'O'), ('at', 'IN', 'O'), ('Google', 'NNP', 'B-ORGANIZATION'), ('.', '.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import *\n",
    "\n",
    "sentence = \"Mark and John are working at Google.\"\n",
    "print(ne_chunk(pos_tag(word_tokenize(sentence))))\n",
    "\n",
    "tagged = ne_chunk(pos_tag(word_tokenize(sentence)))\n",
    "ab = tree2conlltags(tagged)\n",
    "print(ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
